import os
import torch
import datetime
import logging
from pathlib import Path
from torch.utils.data import DataLoader, Subset
from tqdm import tqdm
import torch.nn.functional as F
import math

from models.ddpm_pn import TransformerDDPMRegNet
from utils.ddpm_schedule import DiffusionSchedule
from LiverDataset import LiverDataset

# === é…ç½® ===
LOG_NAME = 'liver_ddpm_experiment'
BATCH_SIZE = 3
NUM_EPOCHS = 300
LR = 1e-3
NUM_POINTS = 1024
DIFFUSION_STEPS = 300
DATA_ROOT = '/mnt/cluster/workspaces/pfeiffemi/V2SData/NewPipeline/100k_nh'
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# === æ—¶é—´æ­¥ä¾èµ–çš„ pred_disp æƒé‡å‡½æ•° ===
def get_pred_disp_weight(t: torch.Tensor, T: int, alpha=5.0):
    """
    ç»™å®šæ—¶é—´æ­¥ tï¼Œè¾“å‡º pred_disp æƒé‡ï¼Œè¶Šå°è¶Šä¾èµ– gt_dispï¼Œè¶Šå¤§è¶Šä¿¡ä»»é¢„æµ‹
    è¿”å›å€¼èŒƒå›´ï¼š0 ~ 1
    """
    step = T - t.float()  # è¶Šé è¿‘ 0ï¼Œstep è¶Šå¤§
    num = 1.0 - torch.exp(-alpha * step / T)
    denom = 1.0 - torch.exp(torch.tensor(-alpha, device=t.device))  # â† ä¿®å¤ç‚¹
    return num / denom  # shape: [B]

def main():
    # === åˆ›å»ºæ—¥å¿—è·¯å¾„ ===
    time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    exp_dir = Path('./log') / LOG_NAME / time_str
    checkpoints_dir = exp_dir / 'checkpoints'
    exp_dir.mkdir(parents=True, exist_ok=True)
    checkpoints_dir.mkdir(exist_ok=True)
    log_file = exp_dir / 'train_log.txt'
    logging.basicConfig(filename=log_file, level=logging.INFO)

    def log(msg):
        print(msg)
        logging.info(msg)

    # === åŠ è½½æ•°æ®é›† ===
    log("ğŸ“¦ åŠ è½½ liver æ•°æ®...")
    dataset = LiverDataset(DATA_ROOT, num_points=NUM_POINTS, preload=False)
    dataset = Subset(dataset, range(5000))  # åªå–å‰nä¸ªæ ·æœ¬
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
    log(f"âœ… æ•°æ®æ ·æœ¬æ•°: {len(dataset)}")

    # === æ¨¡å‹ä¸ä¼˜åŒ–å™¨ ===
    model = TransformerDDPMRegNet(d_model=128, npoint=NUM_POINTS, use_pred_disp=True).to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)

    # === Warmup + Cosine Scheduler ===
    def lr_lambda(epoch):
        if epoch < 10:
            return epoch / 10  # å‰10è½®çº¿æ€§ warmup
        return 0.5 * (1 + math.cos((epoch - 10) / (NUM_EPOCHS - 10) * math.pi))

    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    diffusion = DiffusionSchedule(T=DIFFUSION_STEPS, device=DEVICE)
    best_loss = float('inf')

    # === å¼€å§‹è®­ç»ƒ ===
    for epoch in range(1, NUM_EPOCHS + 1):
        model.train()
        total_loss = 0
        log(f"\nğŸ” Epoch {epoch}/{NUM_EPOCHS}")

        for batch in tqdm(dataloader, desc=f"[Epoch {epoch}]"):
            preop = batch['preop'].to(DEVICE).float()           # [B, N, 3]
            introp = batch['introp'].to(DEVICE).float()         # [B, N, 3]
            gt_disp = batch['displacement'].to(DEVICE).float()  # [B, N, 3]
            disp_mean = batch['disp_mean'].to(DEVICE).float()   # [B, N, 3]
            disp_std = batch['disp_std'].to(DEVICE).float()     # [B, N, 3]

            t = torch.randint(0, diffusion.T, (BATCH_SIZE,), device=DEVICE).long()  # [B]
            x_t, eps = diffusion.add_noise(gt_disp, t)  # [B, N, 3], [B, N, 3]

            with torch.no_grad():
                # === 1. é¢„æµ‹ displacement
                zero_pred = torch.zeros_like(gt_disp)
                x0_pred = model.predict_noise_step(preop, introp, gt_disp*0, x_t, t, pred_disp=zero_pred)
                pred_disp_raw = x0_pred * disp_std + disp_mean  # åæ ‡å‡†åŒ–

                # === 2. åŠ æƒæ··åˆï¼špred_disp vs gt_disp
                w = get_pred_disp_weight(t, diffusion.T, alpha=5.0).view(BATCH_SIZE, 1, 1)  # [B,1,1]
                gt_disp_unnorm = gt_disp * disp_std + disp_mean
                pred_disp = w * pred_disp_raw + (1 - w) * gt_disp_unnorm  # èåˆè¾“å…¥

            # === 3. å‰å‘ä¼ æ’­ ===
            predict_eps_fn = model(preop, introp, gt_disp, t, pred_disp=pred_disp, return_noise=True)
            pred_eps = predict_eps_fn(x_t)

            loss = F.mse_loss(pred_eps, eps)
            total_loss += loss.item()

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        avg_loss = total_loss / len(dataloader)
        log(f"[Epoch {epoch}] å¹³å‡ Loss: {avg_loss:.6f}")
        scheduler.step()
        log(f"[Epoch {epoch}] å½“å‰å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6e}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            save_path = checkpoints_dir / 'best_model.pth'
            log(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹è‡³: {save_path}")
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_loss,
            }, save_path)

    # === ä¿å­˜æœ€ç»ˆæ¨¡å‹ ===
    torch.save(model.state_dict(), checkpoints_dir / 'last_model.pth')
    log("ğŸ è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜æœ€ç»ˆæ¨¡å‹ã€‚")


if __name__ == '__main__':
    main()

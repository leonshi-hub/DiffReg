import os
import datetime
import importlib
import logging
from pathlib import Path

import torch
from torch.utils.data import DataLoader
from LiverDataset import LiverDataset
from tqdm import tqdm

# === å›ºå®šå‚æ•°ï¼ˆä¿ç•™ transformer_3d_tps çš„ flatten æ¥å£ï¼‰===
MODEL_NAME = 'transformer_3d_tps'
BATCH_SIZE = 2            # â›³ æ˜¾å­˜æ§åˆ¶æ ¸å¿ƒï¼šå° batch é¿å… flatten ç‚¸
NPOINT = 1024              # â›³ å°½é‡é¿å…ç”¨ 1024
EPOCHS = 30
LR = 0.001
GPU = '0'
LOG_DIR = 'liver_flatten_safe'
DATA_ROOT = '/mnt/cluster/workspaces/pfeiffemi/V2SData/NewPipeline/100k_nh'

os.environ["CUDA_VISIBLE_DEVICES"] = GPU

def main():
    time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    exp_dir = Path('./log') / LOG_DIR / time_str
    checkpoints_dir = exp_dir / 'checkpoints'
    exp_dir.mkdir(parents=True, exist_ok=True)
    checkpoints_dir.mkdir(exist_ok=True)
    log_file = exp_dir / f'{MODEL_NAME}.txt'
    logger = logging.getLogger(MODEL_NAME)
    logger.setLevel(logging.INFO)
    logger.addHandler(logging.FileHandler(log_file))

    def log(msg): print(msg); logger.info(msg)

    log("âœ… Step 1: åŠ è½½ liver æ•°æ®")
    dataset = LiverDataset(DATA_ROOT, num_points=NPOINT, preload=False)
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
    log(f"æ€»æ ·æœ¬æ•°ï¼š{len(dataset)}")

    log("ğŸ§  Step 2: åŠ è½½æ¨¡å‹")
    MODEL = importlib.import_module(f'models.{MODEL_NAME}')
    model = MODEL.get_model(d_model=128, channel=3, npoint=NPOINT).cuda()
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)

    best_loss = float('inf')

    log("ğŸ¯ Step 3: å¼€å§‹è®­ç»ƒ")
    for epoch in range(1, EPOCHS + 1):
        model.train()
        total_loss = 0
        print(f"ğŸŒŠ Epoch {epoch}/{EPOCHS} å¼€å§‹...")

        for batch in tqdm(loader, desc=f"[Epoch {epoch}]"):
            preop = batch['preop'].cuda().float()
            introp = batch['introp'].cuda().float()

            optimizer.zero_grad()
            warped, loss = model(introp, preop)
            loss.mean().backward()
            optimizer.step()

            total_loss += loss.mean().item()

        avg_loss = total_loss / len(loader)
        log(f"[Epoch {epoch}] å¹³å‡ Loss: {avg_loss:.6f}")

        if avg_loss < best_loss:
            best_loss = avg_loss
            save_path = checkpoints_dir / 'best_model.pth'
            log(f"â†’ ä¿å­˜æœ€ä½³æ¨¡å‹è‡³ {save_path}")
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_loss,
            }, save_path)

    log("ğŸ’¾ è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹")
    torch.save(model.state_dict(), checkpoints_dir / 'last_model.pth')


if __name__ == '__main__':
    main()

import os
import datetime
import importlib
import logging
from pathlib import Path

import torch
from torch.utils.data import DataLoader
from LiverDataset import LiverDataset
from tqdm import tqdm

from torch.nn import functional as F

# === å›ºå®šå‚æ•°ï¼ˆä¿ç•™ transformer_3d_tps çš„ flatten æ¥å£ï¼‰===
MODEL_NAME = 'transformer_3d_tps'
BATCH_SIZE = 2            # â›³ æ˜¾å­˜æ§åˆ¶æ ¸å¿ƒï¼šå° batch é¿å… flatten ç‚¸
NPOINT = 1024              # â›³ å°½é‡é¿å…ç”¨ 1024
EPOCHS = 30
LR = 0.001
GPU = '0'
LOG_DIR = 'liver_flatten_safe'
DATA_ROOT = '/mnt/cluster/workspaces/pfeiffemi/V2SData/NewPipeline/100k_nh'

os.environ["CUDA_VISIBLE_DEVICES"] = GPU



def main():
    time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    exp_dir = Path('./log') / LOG_DIR / time_str
    checkpoints_dir = exp_dir / 'checkpoints'
    exp_dir.mkdir(parents=True, exist_ok=True)
    checkpoints_dir.mkdir(exist_ok=True)
    log_file = exp_dir / f'{MODEL_NAME}.txt'
    logger = logging.getLogger(MODEL_NAME)
    logger.setLevel(logging.INFO)
    logger.addHandler(logging.FileHandler(log_file))

    def log(msg): print(msg); logger.info(msg)

    log("âœ… Step 1: åŠ è½½ liver æ•°æ®")
    dataset = LiverDataset(DATA_ROOT, num_points=NPOINT, preload=False)
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
    log(f"æ€»æ ·æœ¬æ•°ï¼š{len(dataset)}")

    log("ğŸ§  Step 2: åŠ è½½æ¨¡å‹")
    MODEL = importlib.import_module(f'models.{MODEL_NAME}')
    model = MODEL.get_model(d_model=128, channel=3, npoint=NPOINT).cuda()
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)

    best_loss = float('inf')

    log("ğŸ¯ Step 3: å¼€å§‹è®­ç»ƒ")
    for epoch in range(1, EPOCHS + 1):
        model.train()
        total_loss = 0
        print(f"ğŸŒŠ Epoch {epoch}/{EPOCHS} å¼€å§‹...")

        for batch in tqdm(loader, desc=f"[Epoch {epoch}]"):
            preop = batch['preop'].cuda().float()
            introp = batch['introp'].cuda().float()
            gt_disp = batch['displacement'].cuda().float() #lossã€
            target = preop + gt_disp
            target = target.permute(0, 2, 1).cuda()  # [B, 3, N]
            optimizer.zero_grad()
            warped = model(introp, preop)
            #print('warped type:', type(warped))
            #if isinstance(warped, tuple):
               # print('warped tuple content:', [type(x) for x in warped])
            # loss1 = chamfer_loss(encoder_input[:, :3, :].cpu(), warped.permute(0,2,1), ps=N)
            loss = F.mse_loss(warped, target)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            #print(type(warped), isinstance(warped, torch.Tensor))

        avg_loss = total_loss / len(loader)
        log(f"[Epoch {epoch}] å¹³å‡ Loss: {avg_loss:.6f}")

        if avg_loss < best_loss:
            best_loss = avg_loss
            save_path = checkpoints_dir / 'best_model.pth'
            log(f"â†’ ä¿å­˜æœ€ä½³æ¨¡å‹è‡³ {save_path}")
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_loss,
            }, save_path)

    log("ğŸ’¾ è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹")
    torch.save(model.state_dict(), checkpoints_dir / 'last_model.pth')


if __name__ == '__main__':
    main()
